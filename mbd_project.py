# -*- coding: utf-8 -*-
"""MBD_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BeFJ2Iof8uN-qFVnC_H7JpP6XVNIBjMZ
"""

## %pip install catboost
# import the datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf
from sklearn.metrics import mean_absolute_error
from datetime import datetime
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor 
from sklearn.decomposition import PCA

# import the datasets
df = pd.read_csv('energy_weather101.csv')

# check the shape of the datasets
print(df.shape)

"""## Data Exploration"""

print("Return first 5 rows.","\n")

df.head()

print("Return last 5 rows.","\n")
df.tail()

print("get the info on the dataset")
df.info()

print("Descriptive statistics include those that summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values.", "\n")
print(df.describe(), "\n")

# get all the columns that have null value
print("get all the columns that have null value","\n")
print(df.isnull().sum())

# remove generation hydro pumped storage aggregated and forecast wind offshore eday ahead column
df.drop(['generation hydro pumped storage aggregated','forecast wind offshore eday ahead'], axis=1, inplace=True)

# get all the columns that have null value
print("get all the columns that have null value","\n")
print(df.isnull().sum())

# replace all the null values using the mean of their respective column
df.fillna(df.mean(), inplace=True)

# get all the columns that have null value
print("get all the columns that have null value","\n")
print(df.isnull().sum())

"""## Feature Selection"""

# delete all columns that start with generation
df.drop(df.filter(regex='generation'), axis=1, inplace=True)

# print all columns
print(df.head())

# which columns only have 0
print("which columns only have 0","\n")
print(df.isin([0]).sum())

# remove rain_1h, rain_3h, snow_3h and clouds_all
df.drop(['rain_1h','rain_3h','snow_3h','clouds_all'], axis=1, inplace=True)

# check for nan values 
print("check for nan values","\n")
print(df.isnull().sum())

# which columns have string in their 
print("which columns have string in their","\n")
print(df.dtypes)

print(df['weather_main'].unique())

# replace all unique items with numbers in weather_main
# Get unique items in the column
unique_items = df['weather_main'].unique()

# Get the rank of each unique item
ranks = pd.Series(range(1, len(unique_items)+1), index=unique_items)

# Replace items with their corresponding rank
df['weather_main'] = df['weather_main'].map(ranks)

print(df['weather_main'].unique())

print(df['weather_description'].unique())

# remove weather_description column
df.drop(['weather_description'], axis=1, inplace=True)

print(df['weather_icon'].unique())

# remove weather_icon column
df.drop(['weather_icon'], axis=1, inplace=True)

# remove columns that doesn't have any implications on what we want to do
df.drop(['forecast solar day ahead'], axis=1, inplace=True)
df.drop(['forecast wind onshore day ahead'], axis=1, inplace=True)
df.drop(['total load forecast'], axis=1, inplace=True)
df.drop(['price day ahead'], axis=1, inplace=True)
df.drop(['price actual'], axis=1, inplace=True)
df.drop(['city_name'], axis=1, inplace=True)
df.drop(['weather_main'], axis=1, inplace=True)

# which columns have string in their 
print("which columns have string in their","\n")
print(df.dtypes)

df.head()

"""## Data Visualization"""

# Calculate the autocorrelation for different lags
lags = np.arange(1, 24 * 10 + 1)
autocorrelation = [df['total load actual'].autocorr(lag=l) for l in lags]

# Plot the autocorrelation
plt.figure()
plt.plot(lags, autocorrelation)
plt.xlabel("Lag")
plt.ylabel("Autocorrelation")
plt.title("Autocorrelation of Actual total Load")
plt.show()

# Load the data into pandas DataFrame
df2 = df 

# use plot_acf to plot the autocorrelation
df2['Change'] = (df2["total load actual"].diff() / df2["total load actual"].max()) * 100
df2.dropna(inplace=True)
fig, ax = plt.subplots(figsize=(13, 9))
plot_acf(df2['Change'],lags=240,bartlett_confint=False, ax=ax)
ax.axhline(y=0.05, linestyle='--', color='gray', alpha=0.5)
ax.axhline(y=-0.05, linestyle='--', color='gray', alpha=0.5)
plt.xlabel('Lag (in hours)')
plt.ylabel('Autocorrelation')
plt.show()

df4 = df

nums = np.arange(1, 30)
MAEArr = []
for i in nums:
    future = np.convolve(df4['total load actual'], np.ones(i)/i, mode='valid')
    MAEArr.append(100 * mean_absolute_error(future, df4['total load actual'][i-1:]) / np.max(df4['total load actual'][i-1:]))
    

# get the mean absolute error as a percentage to plot the graph
MAEPercentage = 100 * mean_absolute_error(df4['total load actual'][:-1], df4['total load actual'][1:]) / np.max(df4['total load actual'][1:])

# plot the graph
plt.plot(nums, MAEArr, label='Moving average')
plt.xlabel('size')
plt.ylabel('MAE (% of max generation)')
plt.legend()
plt.show()

# print the optimal window
print('Optimal window size:', nums[np.argmin(MAEArr)])

df5 = df

df5['date'] = pd.to_datetime(df5['date'], utc=True)

data = df5.set_index('date')

data = data.interpolate(method='linear')

data = data.shift(freq=pd.DateOffset(hours=1))

data['total load actual'].plot(figsize=(8,6))
plt.xlabel('Date')
plt.ylabel('total load actual (MW)')
plt.title('total load actual - (2015-2019)')
plt.show()

# Estimate the autocorrelation coefficients for 10 days
df6 = df

df6['date'] = pd.to_datetime(df6['date'], utc=True)

data = df6.set_index('date')

plt.figure(figsize=(12,6))
plot_acf(data['total load actual'], lags=240)
plt.xticks(np.arange(0, 240, 24), [0,1,2,3,4,5,6,7,8,9])
plt.xlabel('Lags')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation for 10 days')
plt.show()

df6 = df

df6['date'] = pd.to_datetime(df6['date'], utc=True)

data = df6.set_index('date')

# create time of year variable
data['TimeOfYear'] = (data.index.dayofyear + data.index.hour/24)/365

# plot demand against time of year
plt.scatter(data['TimeOfYear'], data['total load actual'], s=1)
plt.xlabel('Time of Year')
plt.ylabel('Demand')
plt.title('Demand vs Time of Year')
plt.show()

# For each of the 12 months of the year, calculate the average demand and display them as a bar chart, and label them appropriately.
data['Month'] = data.index.month
data.groupby('Month')['total load actual'].mean().plot(kind='bar')
plt.xlabel('Month')
plt.ylabel('Average Demand (MW)')
plt.title('Average Demand - (2015-2019)')
plt.show()

# For each of the 24 hours of the day, calculate the average demand and display them as a bar chart, indicating the different hours of the day.
data['Hour'] = data.index.hour
data.groupby('Hour')['total load actual'].mean().plot(kind='bar')
plt.xlabel('Hour')
plt.ylabel('Average Demand (MW)')
plt.title('Average Demand - (2015-2019)')
plt.show()

# For each of the seven days of the week, calculate the average demand and display them as a bar chart.
data['Day'] = data.index.dayofweek
data.groupby('Day')['total load actual'].mean().plot(kind='bar')
plt.xlabel('Day')
plt.ylabel('Average Demand (MW)')
plt.title('Average Demand of EirGrid System Demand - 2014')
plt.show()

df7 = df

# Combine the date and time columns into a datetime object
df7['timestamp'] = pd.to_datetime(df7['date'], format='%m/%d/%Y %H:%M')

# Calculate the average demand for each hour of the day
df7['hour'] = df7['timestamp'].dt.hour
avg_hourly_demand = df7.groupby('hour')['total load actual'].mean()

# Create a list of days to plot (Monday=0, Sunday=6)
days_to_plot = [0, 1, 2, 3, 4, 5, 6]

# Create a dictionary to store the daily total load actual profiles
daily_profiles = {}

# Calculate the total load actual profile for each day of the week
for day in days_to_plot:
    daily_profiles[day] = df7[df7['timestamp'].dt.dayofweek == day].groupby('hour')['total load actual'].mean()

# Create a plot of the daily demand profiles over 24 hours
fig, ax = plt.subplots(figsize=(9, 6))

# Plot each daily profile
for day in days_to_plot:
    day_name = datetime(2022, 1, day+3).strftime('%A')
    ax.plot(daily_profiles[day], label=day_name)

# Add a title and labels
ax.set_title('Variations of Daily load Profiles over 24 Hours')
ax.set_xlabel('Hour of the Day')
ax.set_ylabel('Energy Demand (MW)')
ax.legend()

# Show the plot
plt.show()

# create a heatmap
# plt.figure(figsize=(20,20))
sns.heatmap(df.corr(), annot=True, cmap='RdYlGn')

fig = plt.figure()
axes1 = plt.subplot2grid((1,1), (0,0))

style.use("ggplot")
sns.lineplot(x= df["date"], y= df["total load actual"], data = df)
sns.set(rc={'figure.figsize': (20,10)})

# plt.figure(figsize=(20,20))

# plt.title("Electricity consumption in Spain 2016-2021")
# plt.xlabel("Date")
# plt.ylabel("Energy in MW")
# plt.grid(True)
# plt.legend()

# Lets us see the Distribution off Energy Consumption so we have a idea about your Dataset a bit more
fig = plt.figure(figsize = (15,10))
sns.distplot(df["total load actual"])
plt.title("Energy Distribution")

"""## Model

Linear Regression
"""

# Linear Regression

# create a linear regression object
lm = LinearRegression()

# get all the columns except total load actual
feature_cols = df.columns.drop('total load actual')

# remove the date column
feature_cols = feature_cols.drop('date')

# remove timestamp
feature_cols = feature_cols.drop('timestamp')

# create X and y
X = df[feature_cols]
y = df['total load actual']

# split the data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# fit the model
lm.fit(X_train, y_train)

# prediction
y_pred = lm.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""Random Forest"""

# build a random forest model
rf = RandomForestRegressor(n_estimators=200, random_state=1)

# fit the model
rf.fit(X_train, y_train)

# prediction
y_pred = rf.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""Gradient Boosting"""

# build a Gradient Boosting model
gb = GradientBoostingRegressor(n_estimators=200, random_state=1)

# fit the model
gb.fit(X_train, y_train)

# prediction
y_pred = gb.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""Support Vector Regression"""

# do Support Vector Regression:
svr = SVR(kernel='rbf', C=1e3, gamma=0.1)

# fit the model
svr.fit(X_train, y_train)

# prediction
y_pred = svr.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""Neural Networks"""

# do Neural Networks
mlp = MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=500, alpha=0.0001,
                        solver='adam', verbose=10,  random_state=21,tol=0.000000001)

# fit the model
mlp.fit(X_train, y_train)

# prediction
y_pred = mlp.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""XGBoost"""

# do XGBoost
xgb = XGBRegressor(n_estimators=200, random_state=1)

# fit the model
xgb.fit(X_train, y_train)

# prediction
y_pred = xgb.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""CatBoost"""

# do CatBoost
cat = CatBoostRegressor(n_estimators=200, random_state=1)

# fit the model
cat.fit(X_train, y_train)

# prediction
y_pred = cat.predict(X_test)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""PCA"""

# perform pca and build linear regression model on it
pca = PCA(n_components=10)

# fit the model
pca.fit(X_train)

# transform the data
X_train_pca = pca.transform(X_train)

# transform the test data
X_test_pca = pca.transform(X_test)

"""Fit the best model we have Random Forest and XGBoost.

Random forest - PCA
"""

# build a random forest model
rf = RandomForestRegressor(n_estimators=200, random_state=1)

# fit the model
rf.fit(X_train_pca, y_train)

# prediction
y_pred = rf.predict(X_test_pca)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

"""XGBoost - PCA"""

# do XGBoost
xgb = XGBRegressor(n_estimators=200, random_state=1)

# fit the model
xgb.fit(X_train_pca, y_train)

# prediction
y_pred = xgb.predict(X_test_pca)

# calculate MAPE
mape = 100 - np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# calculate the mean square error percentage
error = mean_squared_error(y_test, y_pred) / np.mean(y_test)

# print the error
print(error)

import matplotlib.pyplot as plt
#plot the mse of the models
mse=[490.69,210.67,254.92,622.20,515.70,210.89,212.54]
mse.sort()
print(mse)
models = ['Random Forest','XGBoost','CatBoost','Gradient Boosting','Neural Networks','Linear Regression','Support Vector Regression']
#plot the accuracy of each model
plt.figure(figsize=(10,5))
plt.plot(models,mse)
plt.xticks(rotation=270)
plt.title('MSE of each model')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.show()